{"cells":[{"cell_type":"code","execution_count":null,"id":"400e5143","metadata":{"id":"400e5143","outputId":"92915c77-9137-461e-eb7f-9004991ff128"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\joseo\\OneDrive\\바탕 화면\\desktop\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"name":"stdout","output_type":"stream","text":["Train samples: 320457\n","Validation samples: 80115\n","Test samples: 50047 (최종 평가용)\n","Using device: cuda\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1/5 [Train]: 100%|██████████| 20029/20029 [23:37<00:00, 14.13it/s, loss=0.234] \n"]},{"name":"stdout","output_type":"stream","text":["\n","[Epoch 1] Train Loss: 0.1675\n"]},{"name":"stderr","output_type":"stream","text":["                                                               \r"]},{"name":"stdout","output_type":"stream","text":["\n","============================================================\n","Epoch 1 Validation Evaluation Results\n","============================================================\n","Loss: 0.1530\n","Hamming Loss: 0.0521\n","Subset Accuracy (Exact Match): 0.6983\n","Sample Accuracy (Average): 0.9479\n","\n","--- Macro Metrics ---\n","Precision: 0.9535\n","Recall: 0.9887\n","F1-Score: 0.9706\n","\n","--- Micro Metrics ---\n","Precision: 0.9532\n","Recall: 0.9889\n","F1-Score: 0.9707\n","\n","--- Per-Label Metrics ---\n","Label                          Precision    Recall       F1           Support   \n","----------------------------------------------------------------------------\n","linguistic_acceptability       0.9681       0.9946       0.9812       67772     \n","consistency                    0.9532       0.9908       0.9717       70048     \n","interestingness                0.9408       0.9991       0.9691       72682     \n","unbias                         0.9911       0.9794       0.9852       71084     \n","harmlessness                   0.9923       0.9922       0.9923       69380     \n","no_hallucination               0.9113       0.9695       0.9395       63862     \n","understandability              0.9373       0.9860       0.9610       71930     \n","sensibleness                   0.9311       0.9875       0.9584       69283     \n","specificity                    0.9561       0.9991       0.9772       73868     \n","============================================================\n","\n","✓ New best model saved! (Validation F1: 0.9706)\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2/5 [Train]: 100%|██████████| 20029/20029 [23:12<00:00, 14.39it/s, loss=0.177] \n"]},{"name":"stdout","output_type":"stream","text":["\n","[Epoch 2] Train Loss: 0.1444\n"]},{"name":"stderr","output_type":"stream","text":["                                                               \r"]},{"name":"stdout","output_type":"stream","text":["\n","============================================================\n","Epoch 2 Validation Evaluation Results\n","============================================================\n","Loss: 0.1499\n","Hamming Loss: 0.0515\n","Subset Accuracy (Exact Match): 0.7008\n","Sample Accuracy (Average): 0.9485\n","\n","--- Macro Metrics ---\n","Precision: 0.9585\n","Recall: 0.9835\n","F1-Score: 0.9707\n","\n","--- Micro Metrics ---\n","Precision: 0.9583\n","Recall: 0.9839\n","F1-Score: 0.9709\n","\n","--- Per-Label Metrics ---\n","Label                          Precision    Recall       F1           Support   \n","----------------------------------------------------------------------------\n","linguistic_acceptability       0.9712       0.9946       0.9827       67772     \n","consistency                    0.9638       0.9761       0.9699       70048     \n","interestingness                0.9416       0.9986       0.9693       72682     \n","unbias                         0.9917       0.9786       0.9851       71084     \n","harmlessness                   0.9936       0.9908       0.9922       69380     \n","no_hallucination               0.9253       0.9518       0.9383       63862     \n","understandability              0.9479       0.9782       0.9628       71930     \n","sensibleness                   0.9348       0.9841       0.9588       69283     \n","specificity                    0.9565       0.9990       0.9773       73868     \n","============================================================\n","\n","✓ New best model saved! (Validation F1: 0.9707)\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3/5 [Train]: 100%|██████████| 20029/20029 [23:07<00:00, 14.43it/s, loss=0.183] \n"]},{"name":"stdout","output_type":"stream","text":["\n","[Epoch 3] Train Loss: 0.1331\n"]},{"name":"stderr","output_type":"stream","text":["                                                               \r"]},{"name":"stdout","output_type":"stream","text":["\n","============================================================\n","Epoch 3 Validation Evaluation Results\n","============================================================\n","Loss: 0.1483\n","Hamming Loss: 0.0506\n","Subset Accuracy (Exact Match): 0.7049\n","Sample Accuracy (Average): 0.9494\n","\n","--- Macro Metrics ---\n","Precision: 0.9574\n","Recall: 0.9860\n","F1-Score: 0.9713\n","\n","--- Micro Metrics ---\n","Precision: 0.9571\n","Recall: 0.9863\n","F1-Score: 0.9715\n","\n","--- Per-Label Metrics ---\n","Label                          Precision    Recall       F1           Support   \n","----------------------------------------------------------------------------\n","linguistic_acceptability       0.9733       0.9936       0.9834       67772     \n","consistency                    0.9616       0.9820       0.9717       70048     \n","interestingness                0.9414       0.9986       0.9692       72682     \n","unbias                         0.9927       0.9783       0.9855       71084     \n","harmlessness                   0.9942       0.9904       0.9923       69380     \n","no_hallucination               0.9186       0.9635       0.9405       63862     \n","understandability              0.9458       0.9804       0.9628       71930     \n","sensibleness                   0.9319       0.9882       0.9592       69283     \n","specificity                    0.9566       0.9988       0.9772       73868     \n","============================================================\n","\n","✓ New best model saved! (Validation F1: 0.9713)\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4/5 [Train]: 100%|██████████| 20029/20029 [23:11<00:00, 14.39it/s, loss=0.146] \n"]},{"name":"stdout","output_type":"stream","text":["\n","[Epoch 4] Train Loss: 0.1221\n"]},{"name":"stderr","output_type":"stream","text":["                                                               \r"]},{"name":"stdout","output_type":"stream","text":["\n","============================================================\n","Epoch 4 Validation Evaluation Results\n","============================================================\n","Loss: 0.1526\n","Hamming Loss: 0.0511\n","Subset Accuracy (Exact Match): 0.7037\n","Sample Accuracy (Average): 0.9489\n","\n","--- Macro Metrics ---\n","Precision: 0.9554\n","Recall: 0.9875\n","F1-Score: 0.9711\n","\n","--- Micro Metrics ---\n","Precision: 0.9553\n","Recall: 0.9877\n","F1-Score: 0.9712\n","\n","--- Per-Label Metrics ---\n","Label                          Precision    Recall       F1           Support   \n","----------------------------------------------------------------------------\n","linguistic_acceptability       0.9730       0.9945       0.9837       67772     \n","consistency                    0.9603       0.9841       0.9720       70048     \n","interestingness                0.9425       0.9967       0.9689       72682     \n","unbias                         0.9789       0.9896       0.9842       71084     \n","harmlessness                   0.9934       0.9910       0.9922       69380     \n","no_hallucination               0.9125       0.9688       0.9398       63862     \n","understandability              0.9483       0.9789       0.9634       71930     \n","sensibleness                   0.9329       0.9861       0.9588       69283     \n","specificity                    0.9570       0.9976       0.9769       73868     \n","============================================================\n","\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5/5 [Train]: 100%|██████████| 20029/20029 [22:51<00:00, 14.61it/s, loss=0.0818]\n"]},{"name":"stdout","output_type":"stream","text":["\n","[Epoch 5] Train Loss: 0.1111\n"]},{"name":"stderr","output_type":"stream","text":["                                                               \r"]},{"name":"stdout","output_type":"stream","text":["\n","============================================================\n","Epoch 5 Validation Evaluation Results\n","============================================================\n","Loss: 0.1608\n","Hamming Loss: 0.0532\n","Subset Accuracy (Exact Match): 0.6902\n","Sample Accuracy (Average): 0.9468\n","\n","--- Macro Metrics ---\n","Precision: 0.9605\n","Recall: 0.9789\n","F1-Score: 0.9695\n","\n","--- Micro Metrics ---\n","Precision: 0.9604\n","Recall: 0.9794\n","F1-Score: 0.9698\n","\n","--- Per-Label Metrics ---\n","Label                          Precision    Recall       F1           Support   \n","----------------------------------------------------------------------------\n","linguistic_acceptability       0.9732       0.9941       0.9836       67772     \n","consistency                    0.9662       0.9703       0.9683       70048     \n","interestingness                0.9439       0.9928       0.9677       72682     \n","unbias                         0.9891       0.9830       0.9861       71084     \n","harmlessness                   0.9939       0.9900       0.9919       69380     \n","no_hallucination               0.9310       0.9371       0.9341       63862     \n","understandability              0.9506       0.9731       0.9617       71930     \n","sensibleness                   0.9393       0.9744       0.9565       69283     \n","specificity                    0.9575       0.9953       0.9760       73868     \n","============================================================\n","\n","\n","============================================================\n","Loading best model for FINAL evaluation on TEST set...\n","============================================================\n"]},{"name":"stderr","output_type":"stream","text":["                                                               \r"]},{"name":"stdout","output_type":"stream","text":["\n","============================================================\n","Final Test Evaluation Results\n","============================================================\n","Loss: 0.1685\n","Hamming Loss: 0.0546\n","Subset Accuracy (Exact Match): 0.6849\n","Sample Accuracy (Average): 0.9454\n","\n","--- Macro Metrics ---\n","Precision: 0.9592\n","Recall: 0.9786\n","F1-Score: 0.9687\n","\n","--- Micro Metrics ---\n","Precision: 0.9591\n","Recall: 0.9791\n","F1-Score: 0.9690\n","\n","--- Per-Label Metrics ---\n","Label                          Precision    Recall       F1           Support   \n","----------------------------------------------------------------------------\n","linguistic_acceptability       0.9711       0.9943       0.9825       42440     \n","consistency                    0.9660       0.9699       0.9679       43662     \n","interestingness                0.9430       0.9929       0.9673       45411     \n","unbias                         0.9881       0.9827       0.9854       44457     \n","harmlessness                   0.9936       0.9903       0.9919       43331     \n","no_hallucination               0.9291       0.9377       0.9334       39753     \n","understandability              0.9465       0.9698       0.9580       44926     \n","sensibleness                   0.9401       0.9746       0.9570       43241     \n","specificity                    0.9555       0.9956       0.9751       46074     \n","============================================================\n","\n","\n","✓ Best model saved to: C:/Users/joseo/OneDrive/바탕 화면/kobert_multilabel_best.pt\n","  Best Validation F1-Score: 0.9713\n","  Final Test F1-Score: 0.9687\n"]}],"source":["# ===============================\n","# KoBERT Multi-label Training & Evaluation\n","# ===============================\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertModel\n","from kobert_transformers import get_tokenizer\n","from torch.optim import AdamW\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import (\n","    accuracy_score, precision_recall_fscore_support,\n","    hamming_loss, classification_report\n",")\n","from tqdm import tqdm\n","\n","# -------------------------------\n","# 1. 데이터 로드\n","# -------------------------------\n","df_train_split= pd.read_csv(\"/content/drive/MyDrive/data/AI응답 결과에 대한 품질 평가 데이터/train_df.csv\")\n","df_val_split = pd.read_csv(\"/content/drive/MyDrive/data/AI응답 결과에 대한 품질 평가 데이터/val_df.csv\")\n","df_test = pd.read_csv(\"/content/drive/MyDrive/data/AI응답 결과에 대한 품질 평가 데이터/test_df.csv\")\n","\n","label_cols = [\n","    \"linguistic_acceptability\", \"consistency\", \"interestingness\",\n","    \"unbias\", \"harmlessness\", \"no_hallucination\",\n","    \"understandability\", \"sensibleness\", \"specificity\"\n","]\n","\n","# yes / no → 1 / 0 변환\n","for col in label_cols:\n","    df_train_split[col] = df_train_split[col].map({\"yes\": 1, \"no\": 0})\n","    df_val_split[col]  = df_val_split[col].map({\"yes\": 1, \"no\": 0})\n","    df_test[col]  = df_test[col].map({\"yes\": 1, \"no\": 0})\n","\n","df_train_split = df_train_split[[\"text\"] + label_cols].dropna()\n","df_val_split  = df_val_split[[\"text\"] + label_cols].dropna()\n","df_test  = df_test[[\"text\"] + label_cols].dropna()\n","\n","print(f\"Train samples: {len(df_train_split_split)}\")\n","print(f\"Validation samples: {len(df_val_split)}\")\n","print(f\"Test samples: {len(df_test)} (최종 평가용)\")\n","\n","# -------------------------------\n","# 2. Dataset 정의\n","# -------------------------------\n","class KoBERTDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_len=128):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        encoding = self.tokenizer(\n","            self.texts[idx],\n","            padding=\"max_length\",\n","            truncation=True,\n","            max_length=self.max_len,\n","            return_tensors=\"pt\"\n","        )\n","        return {\n","            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n","            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n","            \"labels\": torch.tensor(self.labels[idx], dtype=torch.float)\n","        }\n","\n","# -------------------------------\n","# 3. Tokenizer & DataLoader\n","# -------------------------------\n","tokenizer = get_tokenizer()\n","\n","train_dataset = KoBERTDataset(\n","    df_train_split[\"text\"].tolist(),\n","    df_train_split[label_cols].values.tolist(),\n","    tokenizer\n",")\n","val_dataset = KoBERTDataset(\n","    df_val_split[\"text\"].tolist(),\n","    df_val_split[label_cols].values.tolist(),\n","    tokenizer\n",")\n","test_dataset = KoBERTDataset(\n","    df_test[\"text\"].tolist(),\n","    df_test[label_cols].values.tolist(),\n","    tokenizer\n",")\n","\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=16)\n","test_loader = DataLoader(test_dataset, batch_size=16)\n","\n","# -------------------------------\n","# 4. 모델 정의\n","# -------------------------------\n","class KoBERTMultiLabel(nn.Module):\n","    def __init__(self, num_labels, dropout=0.1):\n","        super().__init__()\n","        self.bert = BertModel.from_pretrained(\"skt/kobert-base-v1\")\n","        self.dropout = nn.Dropout(dropout)\n","        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask\n","        )\n","        pooled_output = self.dropout(outputs.pooler_output)\n","        return self.classifier(pooled_output)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","model = KoBERTMultiLabel(num_labels=len(label_cols)).to(device)\n","\n","# -------------------------------\n","# 5. Optimizer & Loss\n","# -------------------------------\n","optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n","criterion = nn.BCEWithLogitsLoss()\n","\n","# -------------------------------\n","# 6. 평가 함수\n","# -------------------------------\n","def evaluate_model(model, dataloader, threshold=0.5):\n","    \"\"\"\n","    Multi-label classification 평가\n","    Returns: dict with overall and per-label metrics\n","    \"\"\"\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","    all_probs = []\n","    total_loss = 0\n","\n","    with torch.no_grad():\n","        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","            labels = batch[\"labels\"].to(device)\n","\n","            logits = model(input_ids, attention_mask)\n","            loss = criterion(logits, labels)\n","            total_loss += loss.item()\n","\n","            probs = torch.sigmoid(logits)\n","            preds = (probs > threshold).float()\n","\n","            all_probs.append(probs.cpu().numpy())\n","            all_preds.append(preds.cpu().numpy())\n","            all_labels.append(labels.cpu().numpy())\n","\n","    all_preds = np.vstack(all_preds)\n","    all_labels = np.vstack(all_labels)\n","    all_probs = np.vstack(all_probs)\n","\n","    # Overall metrics\n","    avg_loss = total_loss / len(dataloader)\n","    hamming = hamming_loss(all_labels, all_preds)\n","\n","    # Subset accuracy (exact match)\n","    subset_acc = accuracy_score(all_labels, all_preds)\n","\n","    # Per-sample accuracy (at least one correct)\n","    sample_acc = np.mean([\n","        accuracy_score(all_labels[i], all_preds[i])\n","        for i in range(len(all_labels))\n","    ])\n","\n","    # Macro/Micro metrics\n","    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n","        all_labels, all_preds, average='macro', zero_division=0\n","    )\n","    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(\n","        all_labels, all_preds, average='micro', zero_division=0\n","    )\n","\n","    # Per-label metrics\n","    per_label_metrics = {}\n","    for idx, label_name in enumerate(label_cols):\n","        precision, recall, f1, support = precision_recall_fscore_support(\n","            all_labels[:, idx], all_preds[:, idx],\n","            average='binary', zero_division=0\n","        )\n","        per_label_metrics[label_name] = {\n","            'precision': precision,\n","            'recall': recall,\n","            'f1': f1,\n","            'support': int(all_labels[:, idx].sum())\n","        }\n","\n","    results = {\n","        'loss': avg_loss,\n","        'hamming_loss': hamming,\n","        'subset_accuracy': subset_acc,\n","        'sample_accuracy': sample_acc,\n","        'macro': {\n","            'precision': precision_macro,\n","            'recall': recall_macro,\n","            'f1': f1_macro\n","        },\n","        'micro': {\n","            'precision': precision_micro,\n","            'recall': recall_micro,\n","            'f1': f1_micro\n","        },\n","        'per_label': per_label_metrics\n","    }\n","\n","    return results, all_probs\n","\n","def print_evaluation_results(results, phase=\"Test\"):\n","    \"\"\"평가 결과 출력\"\"\"\n","    print(f\"\\n{'='*60}\")\n","    print(f\"{phase} Evaluation Results\")\n","    print(f\"{'='*60}\")\n","    print(f\"Loss: {results['loss']:.4f}\")\n","    print(f\"Hamming Loss: {results['hamming_loss']:.4f}\")\n","    print(f\"Subset Accuracy (Exact Match): {results['subset_accuracy']:.4f}\")\n","    print(f\"Sample Accuracy (Average): {results['sample_accuracy']:.4f}\")\n","\n","    print(f\"\\n--- Macro Metrics ---\")\n","    print(f\"Precision: {results['macro']['precision']:.4f}\")\n","    print(f\"Recall: {results['macro']['recall']:.4f}\")\n","    print(f\"F1-Score: {results['macro']['f1']:.4f}\")\n","\n","    print(f\"\\n--- Micro Metrics ---\")\n","    print(f\"Precision: {results['micro']['precision']:.4f}\")\n","    print(f\"Recall: {results['micro']['recall']:.4f}\")\n","    print(f\"F1-Score: {results['micro']['f1']:.4f}\")\n","\n","    print(f\"\\n--- Per-Label Metrics ---\")\n","    print(f\"{'Label':<30} {'Precision':<12} {'Recall':<12} {'F1':<12} {'Support':<10}\")\n","    print(\"-\" * 76)\n","    for label_name, metrics in results['per_label'].items():\n","        print(f\"{label_name:<30} \"\n","              f\"{metrics['precision']:<12.4f} \"\n","              f\"{metrics['recall']:<12.4f} \"\n","              f\"{metrics['f1']:<12.4f} \"\n","              f\"{metrics['support']:<10}\")\n","    print(f\"{'='*60}\\n\")\n","\n","# -------------------------------\n","# 7. 학습 루프\n","# -------------------------------\n","EPOCHS = 5\n","best_f1 = 0\n","best_model_state = None\n","\n","for epoch in range(EPOCHS):\n","    # Training\n","    model.train()\n","    total_loss = 0\n","\n","    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\")\n","    for batch in progress_bar:\n","        optimizer.zero_grad()\n","\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        labels = batch[\"labels\"].to(device)\n","\n","        logits = model(input_ids, attention_mask)\n","        loss = criterion(logits, labels)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        progress_bar.set_postfix({'loss': loss.item()})\n","\n","    avg_train_loss = total_loss / len(train_loader)\n","    print(f\"\\n[Epoch {epoch+1}] Train Loss: {avg_train_loss:.4f}\")\n","\n","    # Validation\n","    val_results, _ = evaluate_model(model, val_loader)\n","    print_evaluation_results(val_results, phase=f\"Epoch {epoch+1} Validation\")\n","\n","    # Save best model based on validation F1\n","    current_f1 = val_results['macro']['f1']\n","    if current_f1 > best_f1:\n","        best_f1 = current_f1\n","        best_model_state = model.state_dict().copy()\n","        print(f\"✓ New best model saved! (Validation F1: {best_f1:.4f})\")\n","\n","# -------------------------------\n","# 8. 최종 평가 (Test 데이터)\n","# -------------------------------\n","print(\"\\n\" + \"=\"*60)\n","print(\"Loading best model for FINAL evaluation on TEST set...\")\n","print(\"=\"*60)\n","\n","model.load_state_dict(best_model_state)\n","final_results, final_probs = evaluate_model(model, test_loader)\n","print_evaluation_results(final_results, phase=\"Final Test\")\n","\n","# -------------------------------\n","# 9. 모델 저장\n","# -------------------------------\n","SAVE_PATH = \"content/drive/MyDrive/data/AI응답 결과에 대한 품질 평가 데이터/kobert_multilabel_best.pt\"\n","torch.save({\n","    'model_state_dict': best_model_state,\n","    'label_cols': label_cols,\n","    'best_val_f1': best_f1,\n","    'test_results': final_results\n","}, SAVE_PATH)\n","print(f\"\\n✓ Best model saved to: {SAVE_PATH}\")\n","print(f\"  Best Validation F1-Score: {best_f1:.4f}\")\n","print(f\"  Final Test F1-Score: {final_results['macro']['f1']:.4f}\")"]},{"cell_type":"code","execution_count":null,"id":"7c8f0708","metadata":{"id":"7c8f0708"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}